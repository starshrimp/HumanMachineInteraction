{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks\n",
    "\n",
    "## Beginning\n",
    "df.info (), df.head(10)\n",
    "\n",
    "- for binary columns: filtered_df = df[~df['InsulinResistance'].isin([0, 1])] -> to find out whether there are any values other than 0 and 1\n",
    "- data.drop_duplicates(inplace=True) for duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram & Density\n",
    "Helps get an overview of how the data is spread out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the style of seaborn plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Function to plot distribution of each column\n",
    "def plot_distributions(df):\n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[column], kde=False, bins=30)\n",
    "        plt.title(f'Histogram of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        # Density plot (KDE)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.kdeplot(df[column], shade=True)\n",
    "        plt.title(f'Density Plot of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Density')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display basic statistics\n",
    "        print(f'Statistics for {column}:')\n",
    "        print(df[column].describe())\n",
    "        print('\\nSkewness:', df[column].skew())\n",
    "        print('\\nKurtosis:', df[column].kurtosis())\n",
    "        print('\\n')\n",
    "\n",
    "# Plot distributions\n",
    "plot_distributions(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test \n",
    "Detailed Workflow:\n",
    "1. Load Data: Load your raw dataset into a dataframe.\n",
    "2. Initial Split: Perform the train-test split to separate the dataset into training and test sets.\n",
    "3. Imputation on Training Data: Train the imputation model on the training set.\n",
    "4. Apply Imputation: Apply the trained imputation model to both the training and test sets.\n",
    "5. Identify and Fix Issues: Identify and fix issues in the training data and apply the same fixes to the test data.\n",
    "6. Model Training and Evaluation: Train your machine learning model on the training set and evaluate it on the test set.  \n",
    "\n",
    "Key Points:\n",
    "- Consistent Processing: Any cleaning, transformation, or preprocessing step applied to the training data must also be applied to the test data.\n",
    "- Fit on Training, Apply to Both: For steps like scaling or encoding, fit the transformer (e.g., scaler, encoder) on the training data and then apply it to both the training and test data.\n",
    "- No Leakage: Ensure that no information from the test set leaks into the training process to maintain the validity of your model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Plot\n",
    "Components of a Density Plot:\n",
    "- X-axis (Horizontal axis): Represents the range of values for the variable being analyzed.\n",
    "- Y-axis (Vertical axis): Represents the density, which is an estimate of the probability of the variable taking a given value.\n",
    "- Curve: The smooth line represents the estimated density function. The area under the curve sums to 1, indicating that it represents a probability distribution.\n",
    "\n",
    "Key Information from a Density Plot:\n",
    "- Central Tendency: The peaks of the density plot indicate where the data values are concentrated. The highest peak represents the mode (the most common value).\n",
    "- Spread: The width of the curve provides information about the variability of the data. A wider curve indicates more spread out data, while a narrower curve indicates that the data values are closer together.\n",
    "- Skewness: The shape of the curve shows the skewness of the data. If the curve is symmetrical, the data is evenly distributed. If it tails off to one side, the data is skewed in that direction:\n",
    "    - Right-skewed (positive skew): The tail is longer on the right side.\n",
    "    - Left-skewed (negative skew): The tail is longer on the left side.\n",
    "- Multimodality: If the density plot has more than one peak, the data might have multiple modes. This could indicate the presence of subgroups within the data.\n",
    "- Comparison between Groups: When multiple density plots are overlaid, they can be used to compare distributions between different groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kurtosis\n",
    "Kurtosis is a statistical measure that describes the shape of a distribution's tails in relation to its overall shape, particularly focusing on the outliers. It indicates whether the data are heavy-tailed or light-tailed compared to a normal distribution.\n",
    "\n",
    "- Mesokurtic (Kurtosis ≈ 3): This is the kurtosis of a normal distribution. The tails are neither heavy nor light.\n",
    "- Leptokurtic (Kurtosis > 3): This indicates heavier tails. The distribution has more outliers.\n",
    "- Platykurtic (Kurtosis < 3): This indicates lighter tails. The distribution has fewer outliers.\n",
    "\n",
    "In practice, kurtosis is often reported as \"excess kurtosis,\" which is calculated as kurtosis - 3. Thus:\n",
    "\n",
    "Excess Kurtosis ≈ 0: Normal distribution.\n",
    "Excess Kurtosis > 0: Leptokurtic.\n",
    "Excess Kurtosis < 0: Platykurtic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness\n",
    "Skewness measures the asymmetry of the probability distribution of a real-valued random variable. It indicates whether the data distribution is skewed to the left or right:\n",
    "\n",
    "- Positive Skewness (> 0): The right tail is longer; the mass of the distribution is concentrated on the left.\n",
    "- Negative Skewness (< 0): The left tail is longer; the mass of the distribution is concentrated on the right.\n",
    "- Zero Skewness (≈ 0): The distribution is symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Skewness & Kurtosis\n",
    "Skewness:\n",
    "- |Skewness| < 0.5: The data is fairly symmetrical.\n",
    "- 0.5 ≤ |Skewness| < 1: The data is moderately skewed.\n",
    "- |Skewness| ≥ 1: The data is highly skewed.\n",
    "\n",
    "Kurtosis:\n",
    "- Excess Kurtosis between -1 and 1: This range is often considered acceptable or indicative of a relatively normal distribution.\n",
    "- Excess Kurtosis outside of -1 to 1: Indicates significant deviations from normality. Values outside this range suggest heavy or light tails.\n",
    "\n",
    "Practical Considerations\n",
    "- Context: The specific thresholds might vary depending on the field of study. For instance, financial data often exhibit high kurtosis due to the presence of outliers.\n",
    " Data Transformation: If the data is highly skewed or has extreme kurtosis, transformations (like logarithmic or Box-Cox) can sometimes help in normalizing the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy pandas scikit-learn shap matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "# Make sure to replace 'your_data.csv' with your actual data file\n",
    "data = pd.read_csv('diabetes_study_final_data.csv')\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomforest\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "# Make sure to replace 'your_data.csv' with your actual data file\n",
    "data = pd.read_csv('your_data.csv')\n",
    "X = data.drop('target_column', axis=1)\n",
    "y = data['target_column']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training a model (using RandomForest as an example)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Feature Permutation Importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(result.importances[sorted_idx].T, vert=False, labels=X_test.columns[sorted_idx])\n",
    "plt.title(\"Permutation Importance of Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP Summary Plot\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot for all features\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "plt.show()\n",
    "\n",
    "# Detailed SHAP summary plot (useful for insights on feature impact)\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot SHAP values for a single prediction\n",
    "choosen_instance = X_test.iloc[0]\n",
    "shap_values_instance = explainer.shap_values(choosen_instance)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values_instance[1], choosen_instance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "# Replace 'your_data.csv' and 'target_column' with your actual data file and target variable\n",
    "data = pd.read_csv('your_data.csv')\n",
    "X = data.drop('target_column', axis=1)\n",
    "y = data['target_column']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training a Logistic Regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Feature Permutation Importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "# Plotting the feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(result.importances[sorted_idx].T, vert=False, labels=X_test.columns[sorted_idx])\n",
    "plt.title(\"Permutation Importance of Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP Summary Plot\n",
    "# Logistic Regression requires a linear explainer\n",
    "explainer = shap.LinearExplainer(model, X_train, feature_dependence=\"independent\")\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot for all features\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "plt.show()\n",
    "\n",
    "# Detailed SHAP summary plot (useful for insights on feature impact)\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot SHAP values for a single prediction\n",
    "choosen_instance = X_test.iloc[0]\n",
    "shap_values_instance = explainer.shap_values(choosen_instance)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values_instance, choosen_instance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the performance using common classification metrics such as accuracy, precision, recall, F1-score, and ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming predictions and y_test are already defined from your model testing phase\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f'ROC AUC: {roc_auc:.2f}')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions, target_names=['Absent', 'Present']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(rf, X, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "# Print the accuracy for each fold\n",
    "print(\"Accuracy for each fold:\", scores)\n",
    "\n",
    "# Print average accuracy\n",
    "print(\"Average accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X contains your features (including BMI) and y contains your target variable (Cardiovascular Disease)\n",
    "X = df_with_predictions[['BMI']]\n",
    "y = df_with_predictions['CardiovascularDisease']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the SVM model\n",
    "svm_model = SVC(kernel='linear', C=1.0)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Assuming X contains your features (including BMI) and y contains your target variable (Cardiovascular Disease)\n",
    "X = df_with_predictions[['BMI']]\n",
    "y = df_with_predictions['CardiovascularDisease']\n",
    "\n",
    "# Creating the SVM model\n",
    "svm_model = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "cv_scores = cross_val_score(svm_model, X, y, cv=5)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# Calculate and print the mean accuracy\n",
    "print(\"Mean accuracy:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load example data\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define the models and parameters\n",
    "models = {\n",
    "    'RandomForest': (RandomForestClassifier(), {\n",
    "        'n_estimators': [10, 50, 100],\n",
    "        'max_depth': [None, 10, 20, 30]\n",
    "    }),\n",
    "    'LogisticRegression': (LogisticRegression(), {\n",
    "        'C': [0.1, 1, 10]\n",
    "    }),\n",
    "    'SVM': (SVC(), {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': [1, 10, 100]\n",
    "    }),\n",
    "    'XGBoost': (xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Set up outer cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop through each type of model\n",
    "for name, (model, params) in models.items():\n",
    "    # Set up inner cross-validation\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    clf = GridSearchCV(estimator=model, param_grid=params, cv=inner_cv, scoring='accuracy')\n",
    "    \n",
    "    # Perform outer cross-validation\n",
    "    outer_scores = cross_val_score(clf, X, y, cv=outer_cv, scoring='accuracy')\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"{name}: Mean accuracy = {np.mean(outer_scores):.3f} (+/- {np.std(outer_scores):.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading, Exploration, Cleaning, Feature Engineering, Data Transfrmation, Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1. Data Import\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# 2. Data Exploration\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# 3. Data Cleaning\n",
    "# Handle missing values and duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# 4. Feature Engineering (example: creating a new feature)\n",
    "data['new_feature'] = data['feature1'] / data['feature2']\n",
    "\n",
    "# 5. Data Transformation\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['feature1', 'feature2', 'new_feature']\n",
    "categorical_features = ['category1', 'category2']\n",
    "\n",
    "# Define transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# 6. Train-Test Split\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7. Imputation (handled in the transformers above)\n",
    "\n",
    "# 8. Feature Selection (example using all features for simplicity)\n",
    "# 9. Resampling (if needed) (example not shown here)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Your data is now ready for model training\n",
    "print('Training data shape:', X_train.shape)\n",
    "print('Testing data shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation with RandomForest for Numerical and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "# 1. Data Import\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# 2. Data Exploration\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# 3. Data Cleaning\n",
    "# Handle missing values and duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# 4. Train-Test Split\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define transformers\n",
    "\n",
    "# Custom categorical imputer\n",
    "class CategoricalImputer:\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.imputer = IterativeImputer(estimator=RandomForestClassifier(), max_iter=10, random_state=42)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            self.label_encoders[col] = le\n",
    "        self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            le = self.label_encoders[col]\n",
    "            X[col] = le.transform(X[col].astype(str))\n",
    "        X_imputed = pd.DataFrame(self.imputer.transform(X), columns=X.columns)\n",
    "        for col in X.columns:\n",
    "            le = self.label_encoders[col]\n",
    "            X_imputed[col] = le.inverse_transform(X_imputed[col].round().astype(int))\n",
    "        return X_imputed\n",
    "\n",
    "# Numerical Transformer using IterativeImputer with RandomForestRegressor\n",
    "numerical_features = ['feature1', 'feature2', 'new_feature']\n",
    "categorical_features = ['category1', 'category2']\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', CategoricalImputer()),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit the preprocessor on the training data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Your data is now ready for model training\n",
    "print('Training data shape:', X_train_transformed.shape)\n",
    "print('Testing data shape:', X_test_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a RandomForest to impute missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Separate predictors and target in the training data\n",
    "X_train = train_df.drop(columns=['InsulinResistance'])\n",
    "y_train = train_df['InsulinResistance']\n",
    "\n",
    "# Step 3: Remove rows where y_train is NaN for training the imputation model\n",
    "X_train_notna = X_train[y_train.notna()]\n",
    "y_train_notna = y_train[y_train.notna()]\n",
    "\n",
    "# Check for NaNs in y_train_notna\n",
    "print(\"NaNs in y_train_notna:\", y_train_notna.isna().sum())\n",
    "\n",
    "# Ensure y_train_notna contains only 0 or 1\n",
    "print(\"Unique values in y_train_notna:\", y_train_notna.unique())\n",
    "\n",
    "# Step 4: Train the model for imputing the categorical column\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_notna, y_train_notna)\n",
    "\n",
    "# Step 5: Impute missing values in the training set\n",
    "missing_mask_train = y_train.isna()\n",
    "X_train_missing = X_train[missing_mask_train]\n",
    "if not X_train_missing.empty:\n",
    "    train_df.loc[missing_mask_train, 'InsulinResistance'] = rf_model.predict(X_train_missing)\n",
    "\n",
    "# Step 6: Apply the same steps to the test set\n",
    "X_test = test_df.drop(columns=['InsulinResistance'])\n",
    "y_test = test_df['InsulinResistance']\n",
    "missing_mask_test = y_test.isna()\n",
    "X_test_missing = X_test[missing_mask_test]\n",
    "\n",
    "if not X_test_missing.empty:\n",
    "    test_df.loc[missing_mask_test, 'InsulinResistance'] = rf_model.predict(X_test_missing)\n",
    "\n",
    "# Now the train_df and test_df have imputed values for the 'InsulinResistance' column\n",
    "print(train_df)\n",
    "print(test_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
