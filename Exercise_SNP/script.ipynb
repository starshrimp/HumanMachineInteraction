{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks\n",
    "\n",
    "## Beginning\n",
    "df.info (), df.head(10)\n",
    "\n",
    "- for binary columns: filtered_df = df[~df['InsulinResistance'].isin([0, 1])] -> to find out whether there are any values other than 0 and 1\n",
    "- data.drop_duplicates(inplace=True) for duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'Female': 0, 'Male': 1}\n",
    "df['gender'] = df['gender'].map(mapping)\n",
    "\n",
    "\n",
    "mapping = {'Low': 1, 'Moderate': 2, 'High': 3}\n",
    "data['PhysicalActivityLevel'] = data['PhysicalActivityLevel'].map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram & Density\n",
    "Helps get an overview of how the data is spread out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the style of seaborn plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Function to plot distribution of each column\n",
    "def plot_distributions(df):\n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[column], kde=False, bins=30)\n",
    "        plt.title(f'Histogram of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        # Density plot (KDE)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.kdeplot(df[column], fill=True)\n",
    "        plt.title(f'Density Plot of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Density')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display basic statistics\n",
    "        print(f'Statistics for {column}:')\n",
    "        print(df[column].describe())\n",
    "        print('\\nSkewness:', df[column].skew())\n",
    "        print('\\nKurtosis:', df[column].kurtosis())\n",
    "        print('\\n')\n",
    "\n",
    "# Plot distributions\n",
    "plot_distributions(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Plot\n",
    "Components of a Density Plot:\n",
    "- X-axis (Horizontal axis): Represents the range of values for the variable being analyzed.\n",
    "- Y-axis (Vertical axis): Represents the density, which is an estimate of the probability of the variable taking a given value.\n",
    "- Curve: The smooth line represents the estimated density function. The area under the curve sums to 1, indicating that it represents a probability distribution.\n",
    "\n",
    "Key Information from a Density Plot:\n",
    "- Central Tendency: The peaks of the density plot indicate where the data values are concentrated. The highest peak represents the mode (the most common value).\n",
    "- Spread: The width of the curve provides information about the variability of the data. A wider curve indicates more spread out data, while a narrower curve indicates that the data values are closer together.\n",
    "- Skewness: The shape of the curve shows the skewness of the data. If the curve is symmetrical, the data is evenly distributed. If it tails off to one side, the data is skewed in that direction:\n",
    "    - Right-skewed (positive skew): The tail is longer on the right side.\n",
    "    - Left-skewed (negative skew): The tail is longer on the left side.\n",
    "- Multimodality: If the density plot has more than one peak, the data might have multiple modes. This could indicate the presence of subgroups within the data.\n",
    "- Comparison between Groups: When multiple density plots are overlaid, they can be used to compare distributions between different groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kurtosis\n",
    "Kurtosis is a statistical measure that describes the shape of a distribution's tails in relation to its overall shape, particularly focusing on the outliers. It indicates whether the data are heavy-tailed or light-tailed compared to a normal distribution.\n",
    "\n",
    "- Mesokurtic (Kurtosis ≈ 3): This is the kurtosis of a normal distribution. The tails are neither heavy nor light.\n",
    "- Leptokurtic (Kurtosis > 3): This indicates heavier tails. The distribution has more outliers.\n",
    "- Platykurtic (Kurtosis < 3): This indicates lighter tails. The distribution has fewer outliers.\n",
    "\n",
    "In practice, kurtosis is often reported as \"excess kurtosis,\" which is calculated as kurtosis - 3. Thus:\n",
    "\n",
    "Excess Kurtosis ≈ 0: Normal distribution.\n",
    "Excess Kurtosis > 0: Leptokurtic.\n",
    "Excess Kurtosis < 0: Platykurtic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewness\n",
    "Skewness measures the asymmetry of the probability distribution of a real-valued random variable. It indicates whether the data distribution is skewed to the left or right:\n",
    "\n",
    "- Positive Skewness (> 0): The right tail is longer; the mass of the distribution is concentrated on the left.\n",
    "- Negative Skewness (< 0): The left tail is longer; the mass of the distribution is concentrated on the right.\n",
    "- Zero Skewness (≈ 0): The distribution is symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Skewness & Kurtosis\n",
    "Skewness:\n",
    "- |Skewness| < 0.5: The data is fairly symmetrical.\n",
    "- 0.5 ≤ |Skewness| < 1: The data is moderately skewed.\n",
    "- |Skewness| ≥ 1: The data is highly skewed.\n",
    "\n",
    "Kurtosis:\n",
    "- Excess Kurtosis between -1 and 1: This range is often considered acceptable or indicative of a relatively normal distribution.\n",
    "- Excess Kurtosis outside of -1 to 1: Indicates significant deviations from normality. Values outside this range suggest heavy or light tails.\n",
    "\n",
    "Practical Considerations\n",
    "- Context: The specific thresholds might vary depending on the field of study. For instance, financial data often exhibit high kurtosis due to the presence of outliers.\n",
    " Data Transformation: If the data is highly skewed or has extreme kurtosis, transformations (like logarithmic or Box-Cox) can sometimes help in normalizing the distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True, linewidths=.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test \n",
    "Detailed Workflow:\n",
    "1. Load Data: Load your raw dataset into a dataframe.\n",
    "2. Initial Split: Perform the train-test split to separate the dataset into training and test sets.\n",
    "3. Imputation on Training Data: Train the imputation model on the training set.\n",
    "4. Apply Imputation: Apply the trained imputation model to both the training and test sets.\n",
    "5. Identify and Fix Issues: Identify and fix issues in the training data and apply the same fixes to the test data.\n",
    "6. Model Training and Evaluation: Train your machine learning model on the training set and evaluate it on the test set.  \n",
    "\n",
    "Key Points:\n",
    "- Consistent Processing: Any cleaning, transformation, or preprocessing step applied to the training data must also be applied to the test data.\n",
    "- Fit on Training, Apply to Both: For steps like scaling or encoding, fit the transformer (e.g., scaler, encoder) on the training data and then apply it to both the training and test data.\n",
    "- No Leakage: Ensure that no information from the test set leaks into the training process to maintain the validity of your model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split with Export of test data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Define the features and target\n",
    "# Replace 'feature_columns' and 'target_column' with your actual column names\n",
    "X = data.drop(columns=['target_column']) \n",
    "y = data['target_column'] \n",
    "\n",
    "# Perform train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine X_test and y_test into a single DataFrame for exporting\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Export test data to CSV\n",
    "test_data.to_csv('test_data.csv', index=False)\n",
    "\n",
    "# Optionally, you can also export the train data to CSV\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "train_data.to_csv('train_data.csv', index=False)\n",
    "\n",
    "# Optionally, you can delete the test data from your current environment\n",
    "del X_test, y_test, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import, Train-Test Split, Exporting Test, with TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Step 2: Load the dataset (assuming a CSV file)\n",
    "# Replace 'your_dataset.csv' with the path to your actual dataset\n",
    "dataset_path = 'your_dataset.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Step 3: Perform train-test split\n",
    "# Assume the target variable is named 'target'. Change it to your target column name\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Export the test set to a CSV file\n",
    "test_set = pd.concat([X_test, y_test], axis=1)\n",
    "test_set.to_csv('test_set.csv', index=False)\n",
    "\n",
    "# Remove the test set from the dataframe\n",
    "data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Step 5: Use TPOT to find the best model\n",
    "# Initialize TPOTClassifier\n",
    "tpot = TPOTClassifier(verbosity=2, generations=5, population_size=50, random_state=42)\n",
    "\n",
    "# Fit TPOT on the training data\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Export the best model pipeline code\n",
    "tpot.export('tpot_best_model.py')\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "score = tpot.score(X_test, y_test)\n",
    "print(f'Test Accuracy: {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the predicted y for the whole df \n",
    "y_pred = model.predict(X)\n",
    "\n",
    "#adding predicted y to the whole df\n",
    "df_with_predictions = df.copy()\n",
    "df_with_predictions['predictions'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "https://scikit-learn.org/stable/data_transforms.html#data-transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train-Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocesing on the training data -> fitting the imputer on the training data \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# For categorical columns\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train_cat = cat_imputer.fit_transform(X_train[cat_columns])\n",
    "\n",
    "# For numerical columns using a more advanced imputer, e.g., KNNImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "num_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train_num = num_imputer.fit_transform(X_train[num_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. preprocessing on test data with models fit on training data\n",
    "X_test_cat = cat_imputer.transform(X_test[cat_columns])\n",
    "X_test_num = num_imputer.transform(X_test[num_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. combining the numerical and categorical columns into X_train and X_test\n",
    "import numpy as np\n",
    "\n",
    "X_train_processed = np.hstack((X_train_cat, X_train_num))\n",
    "X_test_processed = np.hstack((X_test_cat, X_test_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. further processing e.g. scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_processed)\n",
    "X_test_scaled = scaler.transform(X_test_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. model training \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. model evaluation\n",
    "y_pred = model.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "\n",
    "https://scikit-learn.org/stable/modules/impute.html#impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy pandas scikit-learn shap matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverting & Imputing Mean-Imputed Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "columns_with_mean_imputation = ['BMI', 'BloodSugarLevel']\n",
    "\n",
    "\n",
    "def replace_mean_imputation_with_nan(df, columns, mean_values):\n",
    "    \"\"\"\n",
    "    Replace mean-imputed values with NaN in the specified columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to process.\n",
    "    columns (list of str): The list of column names that had mean imputation applied.\n",
    "    mean_values (dict): A dictionary with column names as keys and mean values as values.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with mean-imputed values replaced with NaN.\n",
    "    \"\"\"\n",
    "    tolerance = 1e-6  # Increase tolerance\n",
    "    for col in columns:\n",
    "        mean_value = mean_values[col]\n",
    "        print(f\"Using mean value for column {col}: {mean_value}\")  # Debug print\n",
    "        df[col] = df[col].apply(lambda x: np.nan if np.abs(x - mean_value) < tolerance else x)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Manually set the observed mean values for the columns\n",
    "mean_values = {\n",
    "    'BMI': 28.052368,\n",
    "    'BloodSugarLevel': 105.053928  # Manually set the observed mean value\n",
    "}\n",
    "\n",
    "# Assuming X_train is your DataFrame\n",
    "X_train = replace_mean_imputation_with_nan(X_train, columns_with_mean_imputation, mean_values)\n",
    "X_test = replace_mean_imputation_with_nan(X_test, columns_with_mean_imputation, mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Identify mean-imputed values and set them to NaN\n",
    "for col in numerical_cols:\n",
    "    mean_value = df[col].mean()\n",
    "    df.loc[df[col] == mean_value, col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn\n",
    "pip install fancyimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make a copy of the DataFrame for each imputation method\n",
    "df_knn = df.copy()\n",
    "df_rf = df.copy()\n",
    "\n",
    "# KNN Imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn[numerical_cols] = knn_imputer.fit_transform(df_knn[numerical_cols])\n",
    "\n",
    "# RandomForest Imputation\n",
    "for col in numerical_cols:\n",
    "    df_rf[col] = impute_with_random_forest(df_rf, col)\n",
    "    \n",
    "def impute_with_random_forest(df, col):\n",
    "    # Separate the data into features and target\n",
    "    X = df.drop(columns=[col])\n",
    "    y = df[col]\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Fit the RandomForest model on the training set\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the missing values\n",
    "    missing_mask = y.isna()\n",
    "    df.loc[missing_mask, col] = rf.predict(df.loc[missing_mask, X.columns])\n",
    "    \n",
    "    return df[col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Make a copy of the DataFrame for each imputation method\n",
    "df_knn = df.copy()\n",
    "df_rf = df.copy()\n",
    "\n",
    "# KNN Imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn[numerical_cols] = knn_imputer.fit_transform(df_knn[numerical_cols])\n",
    "\n",
    "# RandomForest Imputation\n",
    "for col in numerical_cols:\n",
    "    df_rf[col] = impute_with_random_forest(df_rf, col)\n",
    "    \n",
    "def impute_with_random_forest(df, col):\n",
    "    # Separate the data into features and target\n",
    "    X = df.drop(columns=[col])\n",
    "    y = df[col]\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Fit the RandomForest model on the training set\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the missing values\n",
    "    missing_mask = y.isna()\n",
    "    df.loc[missing_mask, col] = rf.predict(df.loc[missing_mask, X.columns])\n",
    "    \n",
    "    return df[col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Original DataFrame with missing values\n",
    "df_original = df.copy()\n",
    "\n",
    "# Mean Squared Error for KNN\n",
    "mse_knn = mean_squared_error(df_original[numerical_cols], df_knn[numerical_cols])\n",
    "\n",
    "# Mean Squared Error for RandomForest\n",
    "mse_rf = mean_squared_error(df_original[numerical_cols], df_rf[numerical_cols])\n",
    "\n",
    "print(f'MSE for KNN Imputation: {mse_knn}')\n",
    "print(f'MSE for RandomForest Imputation: {mse_rf}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing KNN and RandomForestImputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming X_train and X_test are already defined\n",
    "# Ensure the columns are in the correct order\n",
    "\n",
    "# Step 1: Initialize the imputers\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "rf_imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=0)\n",
    "\n",
    "# Step 2: Fit and transform the training data\n",
    "X_train_knn_imputed = pd.DataFrame(knn_imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_train_rf_imputed = pd.DataFrame(rf_imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "# Step 3: Transform the test data using the fitted imputers\n",
    "X_test_knn_imputed = pd.DataFrame(knn_imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "X_test_rf_imputed = pd.DataFrame(rf_imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Optional: Compare imputations\n",
    "# Here you can compare the results of the imputations if you have a ground truth or if you want to check the distributions\n",
    "\n",
    "def compare_imputations(original, knn_imputed, rf_imputed, columns):\n",
    "    comparison = pd.DataFrame({\n",
    "        'Original': original[columns].mean(),\n",
    "        'KNN Imputed': knn_imputed[columns].mean(),\n",
    "        'RF Imputed': rf_imputed[columns].mean()\n",
    "    })\n",
    "    return comparison\n",
    "\n",
    "# Specify columns with NaN values\n",
    "columns_with_nan = ['BMI', 'BloodSugarLevel']\n",
    "\n",
    "comparison_train = compare_imputations(X_train, X_train_knn_imputed, X_train_rf_imputed, columns_with_nan)\n",
    "comparison_test = compare_imputations(X_test, X_test_knn_imputed, X_test_rf_imputed, columns_with_nan)\n",
    "\n",
    "print(\"Comparison of Imputations on Training Data:\")\n",
    "print(comparison_train)\n",
    "print(\"\\nComparison of Imputations on Test Data:\")\n",
    "print(comparison_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "# Make sure to replace 'your_data.csv' with your actual data file\n",
    "data = pd.read_csv('diabetes_study_final_data.csv')\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomforest\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "# Make sure to replace 'your_data.csv' with your actual data file\n",
    "data = pd.read_csv('your_data.csv')\n",
    "X = data.drop('target_column', axis=1)\n",
    "y = data['target_column']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training a model (using RandomForest as an example)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Feature Permutation Importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(result.importances[sorted_idx].T, vert=False, labels=X_test.columns[sorted_idx])\n",
    "plt.title(\"Permutation Importance of Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP Summary Plot\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot for all features\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "plt.show()\n",
    "\n",
    "# Detailed SHAP summary plot (useful for insights on feature impact)\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot SHAP values for a single prediction\n",
    "choosen_instance = X_test.iloc[0]\n",
    "shap_values_instance = explainer.shap_values(choosen_instance)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[1], shap_values_instance[1], choosen_instance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "# Replace 'your_data.csv' and 'target_column' with your actual data file and target variable\n",
    "data = pd.read_csv('your_data.csv')\n",
    "X = data.drop('target_column', axis=1)\n",
    "y = data['target_column']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training a Logistic Regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Feature Permutation Importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "# Plotting the feature importances\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(result.importances[sorted_idx].T, vert=False, labels=X_test.columns[sorted_idx])\n",
    "plt.title(\"Permutation Importance of Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP Summary Plot\n",
    "# Logistic Regression requires a linear explainer\n",
    "explainer = shap.LinearExplainer(model, X_train, feature_dependence=\"independent\")\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot for all features\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "plt.show()\n",
    "\n",
    "# Detailed SHAP summary plot (useful for insights on feature impact)\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "plt.show()\n",
    "\n",
    "# Compute and plot SHAP values for a single prediction\n",
    "choosen_instance = X_test.iloc[0]\n",
    "shap_values_instance = explainer.shap_values(choosen_instance)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values_instance, choosen_instance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the performance using common classification metrics such as accuracy, precision, recall, F1-score, and ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming predictions and y_test are already defined from your model testing phase\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print(f'ROC AUC: {roc_auc:.2f}')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions, target_names=['Absent', 'Present']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(rf, X, y, cv=5)  # 5-fold cross-validation\n",
    "\n",
    "# Print the accuracy for each fold\n",
    "print(\"Accuracy for each fold:\", scores)\n",
    "\n",
    "# Print average accuracy\n",
    "print(\"Average accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X contains your features (including BMI) and y contains your target variable (Cardiovascular Disease)\n",
    "X = df_with_predictions[['BMI']]\n",
    "y = df_with_predictions['CardiovascularDisease']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the SVM model\n",
    "svm_model = SVC(kernel='linear', C=1.0)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Assuming X contains your features (including BMI) and y contains your target variable (Cardiovascular Disease)\n",
    "X = df_with_predictions[['BMI']]\n",
    "y = df_with_predictions['CardiovascularDisease']\n",
    "\n",
    "# Creating the SVM model\n",
    "svm_model = SVC(kernel='linear', C=1.0)\n",
    "\n",
    "# Performing 5-fold cross-validation\n",
    "cv_scores = cross_val_score(svm_model, X, y, cv=5)\n",
    "\n",
    "# Print cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# Calculate and print the mean accuracy\n",
    "print(\"Mean accuracy:\", cv_scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load example data\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define the models and parameters\n",
    "models = {\n",
    "    'RandomForest': (RandomForestClassifier(), {\n",
    "        'n_estimators': [10, 50, 100],\n",
    "        'max_depth': [None, 10, 20, 30]\n",
    "    }),\n",
    "    'LogisticRegression': (LogisticRegression(), {\n",
    "        'C': [0.1, 1, 10]\n",
    "    }),\n",
    "    'SVM': (SVC(), {\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'C': [1, 10, 100]\n",
    "    }),\n",
    "    'XGBoost': (xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Set up outer cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop through each type of model\n",
    "for name, (model, params) in models.items():\n",
    "    # Set up inner cross-validation\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    clf = GridSearchCV(estimator=model, param_grid=params, cv=inner_cv, scoring='accuracy')\n",
    "    \n",
    "    # Perform outer cross-validation\n",
    "    outer_scores = cross_val_score(clf, X, y, cv=outer_cv, scoring='accuracy')\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"{name}: Mean accuracy = {np.mean(outer_scores):.3f} (+/- {np.std(outer_scores):.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an alternate version after getting an error indicating that the LogisticRegression model is not converging within the default number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "models = {\n",
    "    'RandomForest': (RandomForestClassifier(), {\n",
    "        'n_estimators': [10, 50, 100],\n",
    "        'max_depth': [None, 10, 20, 30]\n",
    "    }),\n",
    "    'LogisticRegression': (Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(max_iter=1000))\n",
    "    ]), {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__solver': ['lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    }),\n",
    "    'SVM': (Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', SVC())\n",
    "    ]), {\n",
    "        'classifier__kernel': ['linear', 'rbf'],\n",
    "        'classifier__C': [1, 10, 100]\n",
    "    }),\n",
    "    'XGBoost': (xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    })\n",
    "}\n",
    "\n",
    "# Set up outer cross-validation\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Loop through each type of model\n",
    "for name, (model, params) in models.items():\n",
    "    # Set up inner cross-validation\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    clf = GridSearchCV(estimator=model, param_grid=params, cv=inner_cv, scoring='accuracy')\n",
    "    \n",
    "    # Perform outer cross-validation\n",
    "    outer_scores = cross_val_score(clf, X_train, y_train, cv=outer_cv, scoring='accuracy')\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"{name}: Mean accuracy = {np.mean(outer_scores):.3f} (+/- {np.std(outer_scores):.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading, Exploration, Cleaning, Feature Engineering, Data Transfrmation, Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1. Data Import\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# 2. Data Exploration\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# 3. Data Cleaning\n",
    "# Handle missing values and duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# 4. Feature Engineering (example: creating a new feature)\n",
    "data['new_feature'] = data['feature1'] / data['feature2']\n",
    "\n",
    "# 5. Data Transformation\n",
    "# Define numerical and categorical features\n",
    "numerical_features = ['feature1', 'feature2', 'new_feature']\n",
    "categorical_features = ['category1', 'category2']\n",
    "\n",
    "# Define transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# 6. Train-Test Split\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7. Imputation (handled in the transformers above)\n",
    "\n",
    "# 8. Feature Selection (example using all features for simplicity)\n",
    "# 9. Resampling (if needed) (example not shown here)\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "# Your data is now ready for model training\n",
    "print('Training data shape:', X_train.shape)\n",
    "print('Testing data shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation with RandomForest for Numerical and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "# 1. Data Import\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "# 2. Data Exploration\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "print(data.describe())\n",
    "\n",
    "# 3. Data Cleaning\n",
    "# Handle missing values and duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# 4. Train-Test Split\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define transformers\n",
    "\n",
    "# Custom categorical imputer\n",
    "class CategoricalImputer:\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.imputer = IterativeImputer(estimator=RandomForestClassifier(), max_iter=10, random_state=42)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col].astype(str))\n",
    "            self.label_encoders[col] = le\n",
    "        self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in X.columns:\n",
    "            le = self.label_encoders[col]\n",
    "            X[col] = le.transform(X[col].astype(str))\n",
    "        X_imputed = pd.DataFrame(self.imputer.transform(X), columns=X.columns)\n",
    "        for col in X.columns:\n",
    "            le = self.label_encoders[col]\n",
    "            X_imputed[col] = le.inverse_transform(X_imputed[col].round().astype(int))\n",
    "        return X_imputed\n",
    "\n",
    "# Numerical Transformer using IterativeImputer with RandomForestRegressor\n",
    "numerical_features = ['feature1', 'feature2', 'new_feature']\n",
    "categorical_features = ['category1', 'category2']\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', CategoricalImputer()),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit the preprocessor on the training data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Your data is now ready for model training\n",
    "print('Training data shape:', X_train_transformed.shape)\n",
    "print('Testing data shape:', X_test_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a RandomForest to impute missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the data into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Separate predictors and target in the training data\n",
    "X_train = train_df.drop(columns=['InsulinResistance'])\n",
    "y_train = train_df['InsulinResistance']\n",
    "\n",
    "# Step 3: Remove rows where y_train is NaN for training the imputation model\n",
    "X_train_notna = X_train[y_train.notna()]\n",
    "y_train_notna = y_train[y_train.notna()]\n",
    "\n",
    "# Check for NaNs in y_train_notna\n",
    "print(\"NaNs in y_train_notna:\", y_train_notna.isna().sum())\n",
    "\n",
    "# Ensure y_train_notna contains only 0 or 1\n",
    "print(\"Unique values in y_train_notna:\", y_train_notna.unique())\n",
    "\n",
    "# Step 4: Train the model for imputing the categorical column\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_notna, y_train_notna)\n",
    "\n",
    "# Step 5: Impute missing values in the training set\n",
    "missing_mask_train = y_train.isna()\n",
    "X_train_missing = X_train[missing_mask_train]\n",
    "if not X_train_missing.empty:\n",
    "    train_df.loc[missing_mask_train, 'InsulinResistance'] = rf_model.predict(X_train_missing)\n",
    "\n",
    "# Step 6: Apply the same steps to the test set\n",
    "X_test = test_df.drop(columns=['InsulinResistance'])\n",
    "y_test = test_df['InsulinResistance']\n",
    "missing_mask_test = y_test.isna()\n",
    "X_test_missing = X_test[missing_mask_test]\n",
    "\n",
    "if not X_test_missing.empty:\n",
    "    test_df.loc[missing_mask_test, 'InsulinResistance'] = rf_model.predict(X_test_missing)\n",
    "\n",
    "# Now the train_df and test_df have imputed values for the 'InsulinResistance' column\n",
    "print(train_df)\n",
    "print(test_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
