{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Machine Interaction & Bias Mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Analysis\n",
    "- Loading the dataset using Pandas and performing an initial analysis to understand the basic properties. \n",
    "- Using descriptive statistics and visualizations to identify distributions, detect missing values, and spot outliers.\n",
    "- Identifying the datatypes of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('patient_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rs1047763</th>\n",
       "      <th>rs9282541</th>\n",
       "      <th>rs3827760</th>\n",
       "      <th>rs4988235</th>\n",
       "      <th>rs1801133</th>\n",
       "      <th>rs9374842</th>\n",
       "      <th>BMI</th>\n",
       "      <th>CardiovascularDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.607859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.651948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.885502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.353686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.630251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.243031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.634838</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.809607</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.471339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23.231168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rs1047763  rs9282541  rs3827760  rs4988235  rs1801133  rs9374842  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          1          1          0          0   \n",
       "2          1          1          1          0          0          1   \n",
       "3          0          0          1          0          0          0   \n",
       "4          1          1          0          0          0          0   \n",
       "5          0          0          0          0          0          0   \n",
       "6          1          0          1          0          0          0   \n",
       "7          1          1          1          1          0          0   \n",
       "8          0          0          0          0          0          1   \n",
       "9          0          0          0          0          1          1   \n",
       "\n",
       "         BMI  CardiovascularDisease  \n",
       "0  28.607859                      0  \n",
       "1  26.651948                      0  \n",
       "2  31.885502                      0  \n",
       "3  29.353686                      0  \n",
       "4  33.630251                      0  \n",
       "5  28.243031                      0  \n",
       "6  21.634838                      0  \n",
       "7  36.809607                      1  \n",
       "8  23.471339                      0  \n",
       "9  23.231168                      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rs1047763</th>\n",
       "      <th>rs9282541</th>\n",
       "      <th>rs3827760</th>\n",
       "      <th>rs4988235</th>\n",
       "      <th>rs1801133</th>\n",
       "      <th>rs9374842</th>\n",
       "      <th>BMI</th>\n",
       "      <th>CardiovascularDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.286667</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>28.899291</td>\n",
       "      <td>0.113333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496364</td>\n",
       "      <td>0.469778</td>\n",
       "      <td>0.500735</td>\n",
       "      <td>0.465953</td>\n",
       "      <td>0.452960</td>\n",
       "      <td>0.448098</td>\n",
       "      <td>5.171930</td>\n",
       "      <td>0.317529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.798057</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.292649</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.185791</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.131210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44.188743</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rs1047763   rs9282541   rs3827760   rs4988235   rs1801133   rs9374842  \\\n",
       "count  300.000000  300.000000  300.000000  300.000000  300.000000  300.000000   \n",
       "mean     0.433333    0.326667    0.490000    0.316667    0.286667    0.276667   \n",
       "std      0.496364    0.469778    0.500735    0.465953    0.452960    0.448098   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "              BMI  CardiovascularDisease  \n",
       "count  300.000000             300.000000  \n",
       "mean    28.899291               0.113333  \n",
       "std      5.171930               0.317529  \n",
       "min     13.798057               0.000000  \n",
       "25%     25.292649               0.000000  \n",
       "50%     29.185791               0.000000  \n",
       "75%     32.131210               0.000000  \n",
       "max     44.188743               1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rs1047763                  int64\n",
       "rs9282541                  int64\n",
       "rs3827760                  int64\n",
       "rs4988235                  int64\n",
       "rs1801133                  int64\n",
       "rs9374842                  int64\n",
       "BMI                      float64\n",
       "CardiovascularDisease      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Cleaning\n",
    "Getting an overview over the problems in the data, using the VS Code Extension Data Wrangler from Microsoft, then fix them.\n",
    "\n",
    "- Preparing Data\n",
    "- Encoding categorical variables as needed, preparing data for Model \n",
    "\n",
    "NOT including any ML-method of imputation in this step as I haven't performed the train-test split yet and fitting the imputer on the whole df could risk data leakage -> will do this after EDA/Train-Test-Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be implemented after using DataWrangler VS Code extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis before Imputation \n",
    "Gettng an overview of the data -> how is the data spread out. \n",
    "For each column:\n",
    "- Histogram: display the frequency of data points within specified bins, providing a visual representation of the distribution of a dataset.\n",
    "- Density Plot:  visualize the distribution of data by estimating the probability density function, showing where values are concentrated -> represent probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms / Density Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Set the style of seaborn plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Function to plot distribution of each column\n",
    "def plot_distributions(df):\n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[column], kde=False, bins=30)\n",
    "        plt.title(f'Histogram of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        # Density plot (KDE)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.kdeplot(df[column], fill=True)\n",
    "        plt.title(f'Density Plot of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Density')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display basic statistics\n",
    "        print(f'Statistics for {column}:')\n",
    "        print(df[column].describe())\n",
    "        print('\\nSkewness:', df[column].skew())\n",
    "        print('\\nKurtosis:', df[column].kurtosis())\n",
    "        print('\\n')\n",
    "\n",
    "# Plot distributions\n",
    "plot_distributions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True, linewidths=.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "Using the train_test_split from sklearn.model_selection to split the data\n",
    "- split into train and interim test set\n",
    "- split interim test set into val and test set\n",
    "- export test set to csv\n",
    "- delete interim_test_set and test_set from notebook -> helps ensure prevention of data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 70-30 train-test split, with interim test set\n",
    "train_set, interim_test_set = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_set.drop('targetvariable', axis=1)\n",
    "y_train = train_set['targetvariable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second split: 50-50 validation-test split\n",
    "val_set, test_set = train_test_split(interim_test_set, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = train_set.drop('targetvariable', axis=1)\n",
    "y_val = train_set['targetvariable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the test set to a CSV file\n",
    "test_set.to_csv('test_set.csv', index=False)\n",
    "\n",
    "del test_set, interim_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imputation(X, y, imputer, model):\n",
    "    # Set up the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', imputer),\n",
    "        ('scaler', StandardScaler()),  # Scaling is applied after imputation\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Cross-validate the pipeline\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=kf)\n",
    "    \n",
    "    # Return average MSE\n",
    "    return np.mean(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Initialize the Iterative Imputer using RandomForest\n",
    "rf_imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=10, random_state=42), random_state=42)\n",
    "\n",
    "# Initialize the regression model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate KNN imputer\n",
    "mse_knn = evaluate_imputation(X_train, y_train, knn_imputer, model)\n",
    "print(f\"KNN Imputer MSE: {mse_knn}\")\n",
    "\n",
    "# Evaluate Random Forest imputer (IterativeImputer)\n",
    "mse_rf = evaluate_imputation(X_train, y_train, rf_imputer, model)\n",
    "print(f\"Random Forest Imputer MSE: {mse_rf}\")\n",
    "\n",
    "# Determine the best imputer\n",
    "best_imputer = 'KNN' if mse_knn < mse_rf else 'Random Forest'\n",
    "print(f\"Best imputer selected based on cross-validated MSE: {best_imputer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the selected Imputer on the Training & Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO - Implement the best imputer - delete other imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit KNN imputer on the training data\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train_imputed = knn_imputer.fit_transform(X_train)\n",
    "X_val_imputed = knn_imputer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Iterative Imputer using RandomForest on the training data\n",
    "rf_imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=10, random_state=42), random_state=42)\n",
    "X_train_imputed = rf_imputer.fit_transform(X_train)\n",
    "X_val_imputed = rf_imputer.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection & Training with TPOT\n",
    "Using TPOTClassifier (Tree-based Pipeline Optimization Tool) to automate the selection and training of the best predictive model based on the cleaned training dataset. This tool explores various models and hyperparameter settings to find the optimal solution. As TPOT can also Impute missing values, I am trying out these two things:\n",
    "- TPOT Classifier on Data without Imputation\n",
    "- TPOT Classifier on Data with Imputation from previous step\n",
    "In order to find out which has better scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT with dataframe with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the TPOT classifier\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, scoring='accuracy', random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the validation set\n",
    "print(\"Validation Accuracy: \", tpot.score(X_val, y_val))\n",
    "\n",
    "# Export the best model\n",
    "tpot.export('tpot_best_model.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT with imputed dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the TPOT classifier\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, scoring='accuracy', random_state=42)\n",
    "tpot.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Evaluate the classifier on the validation set\n",
    "print(\"Validation Accuracy: \", tpot.score(X_val_imputed, y_val))\n",
    "\n",
    "# Export the best model\n",
    "tpot.export('tpot_best_model.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "In this step, I analyse the output the best model from the TPOT Classifier using various metrics and visualize results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'model' is your trained model and 'X_val' and 'y_val' are your validation dataset features and target respectively\n",
    "y_pred = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(y_val, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Neg', 'Predicted Pos'], yticklabels=['Actual Neg', 'Actual Pos'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC-AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires that your model supports probability estimates via predict_proba and the target variable is binary\n",
    "y_prob = model.predict_proba(X_val)[:, 1]  # get the probability of the positive class\n",
    "roc_auc = roc_auc_score(y_val, y_prob)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP Summary Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(model, X_val, y_val, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.boxplot(result.importances[sorted_idx].T,\n",
    "            vert=False, labels=X_val.columns[sorted_idx])\n",
    "plt.title('Permutation Importance of each feature')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Identification and Mitigation\n",
    "I am using Fairlearn, a library that was initially developed by researchers at Microsoft. It is now mainained as an open-source project to aid in assessing and mitigating fairness issues in AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE (Oversampling)\n",
    "Oversamples the minority class with synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f'Original dataset shape {Counter(y_train)}')\n",
    "print(f'Resampled dataset shape {Counter(y_smote)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NearMiss (Undersampling)\n",
    "Undersamples instances that are particularly close to instances of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying NearMiss\n",
    "nm = NearMiss(version=1)\n",
    "X_nm, y_nm = nm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f'Original dataset shape {Counter(y_train)}')\n",
    "print(f'Resampled dataset shape {Counter(y_nm)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class-weight Balancing\n",
    "Balances the weight of the individual classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
