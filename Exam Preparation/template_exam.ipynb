{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Machine Interaction & Bias Mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "\n",
    "from fairlearn.reductions import ExponentiatedGradient, EqualizedOdds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Analysis\n",
    "- Loading the dataset using Pandas and performing an initial analysis to understand the basic properties. \n",
    "- Using descriptive statistics and visualizations to identify distributions, detect missing values, and spot outliers.\n",
    "- Identifying the datatypes of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('patient_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rs1047763</th>\n",
       "      <th>rs9282541</th>\n",
       "      <th>rs3827760</th>\n",
       "      <th>rs4988235</th>\n",
       "      <th>rs1801133</th>\n",
       "      <th>rs9374842</th>\n",
       "      <th>BMI</th>\n",
       "      <th>CardiovascularDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.607859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.651948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.885502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.353686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.630251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.243031</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21.634838</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.809607</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.471339</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>23.231168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rs1047763  rs9282541  rs3827760  rs4988235  rs1801133  rs9374842  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          1          1          0          0   \n",
       "2          1          1          1          0          0          1   \n",
       "3          0          0          1          0          0          0   \n",
       "4          1          1          0          0          0          0   \n",
       "5          0          0          0          0          0          0   \n",
       "6          1          0          1          0          0          0   \n",
       "7          1          1          1          1          0          0   \n",
       "8          0          0          0          0          0          1   \n",
       "9          0          0          0          0          1          1   \n",
       "\n",
       "         BMI  CardiovascularDisease  \n",
       "0  28.607859                      0  \n",
       "1  26.651948                      0  \n",
       "2  31.885502                      0  \n",
       "3  29.353686                      0  \n",
       "4  33.630251                      0  \n",
       "5  28.243031                      0  \n",
       "6  21.634838                      0  \n",
       "7  36.809607                      1  \n",
       "8  23.471339                      0  \n",
       "9  23.231168                      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rs1047763</th>\n",
       "      <th>rs9282541</th>\n",
       "      <th>rs3827760</th>\n",
       "      <th>rs4988235</th>\n",
       "      <th>rs1801133</th>\n",
       "      <th>rs9374842</th>\n",
       "      <th>BMI</th>\n",
       "      <th>CardiovascularDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.286667</td>\n",
       "      <td>0.276667</td>\n",
       "      <td>28.899291</td>\n",
       "      <td>0.113333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496364</td>\n",
       "      <td>0.469778</td>\n",
       "      <td>0.500735</td>\n",
       "      <td>0.465953</td>\n",
       "      <td>0.452960</td>\n",
       "      <td>0.448098</td>\n",
       "      <td>5.171930</td>\n",
       "      <td>0.317529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.798057</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.292649</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.185791</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.131210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44.188743</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        rs1047763   rs9282541   rs3827760   rs4988235   rs1801133   rs9374842  \\\n",
       "count  300.000000  300.000000  300.000000  300.000000  300.000000  300.000000   \n",
       "mean     0.433333    0.326667    0.490000    0.316667    0.286667    0.276667   \n",
       "std      0.496364    0.469778    0.500735    0.465953    0.452960    0.448098   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "              BMI  CardiovascularDisease  \n",
       "count  300.000000             300.000000  \n",
       "mean    28.899291               0.113333  \n",
       "std      5.171930               0.317529  \n",
       "min     13.798057               0.000000  \n",
       "25%     25.292649               0.000000  \n",
       "50%     29.185791               0.000000  \n",
       "75%     32.131210               0.000000  \n",
       "max     44.188743               1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rs1047763                  int64\n",
       "rs9282541                  int64\n",
       "rs3827760                  int64\n",
       "rs4988235                  int64\n",
       "rs1801133                  int64\n",
       "rs9374842                  int64\n",
       "BMI                      float64\n",
       "CardiovascularDisease      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints the number of distinct values in each column\n",
    "for column in df.columns:\n",
    "    num_distinct_values = df[column].nunique()\n",
    "    print(f\"Number of distinct values in {column}: {num_distinct_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Data Cleaning\n",
    "Getting an overview over the problems in the data, using the VS Code Extension Data Wrangler from Microsoft, then fix them.\n",
    "\n",
    "- Preparing Data\n",
    "- LabelEncoding categorical variables as needed, preparing data for Model \n",
    "\n",
    "NOT including any ML-method of imputation in this step as I haven't performed the train-test split yet and fitting the imputer on the whole df could risk data leakage -> will do this after EDA/Train-Test-Split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df_le = df\n",
    "\n",
    "columns_to_encode = ['gender', 'Ethnicity', 'Socioeconomic Status', 'AppointmentNoshow']\n",
    "for column in columns_to_encode:\n",
    "    df_le[f'{column}_encoded'] = label_encoder.fit_transform(df_le[column])\n",
    "df_le = df_le.drop(columns=columns_to_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be implemented after using DataWrangler VS Code extension\n",
    "\n",
    "\n",
    "#should return df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis before Imputation \n",
    "Gettng an overview of the data -> how is the data spread out. \n",
    "For each column:\n",
    "- Histogram: display the frequency of data points within specified bins, providing a visual representation of the distribution of a dataset.\n",
    "- Density Plot:  visualize the distribution of data by estimating the probability density function, showing where values are concentrated -> represent probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms / Density Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Function to plot distribution of each column\n",
    "def plot_distributions(df):\n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Histogram\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[column], kde=False, bins=30)\n",
    "        plt.title(f'Histogram of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        # Density plot (KDE)\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.kdeplot(df[column], fill=True)\n",
    "        plt.title(f'Density Plot of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Density')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Display basic statistics\n",
    "        print(f'Statistics for {column}:')\n",
    "        print(df[column].describe())\n",
    "        print('\\nSkewness:', df[column].skew())\n",
    "        print('\\nKurtosis:', df[column].kurtosis())\n",
    "        print('\\n')\n",
    "\n",
    "# Plot distributions\n",
    "plot_distributions(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_clean.corr()\n",
    "\n",
    "# Plot the correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True, linewidths=.5)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "Using the train_test_split from sklearn.model_selection to split the data\n",
    "- split into train and interim test set\n",
    "- split interim test set into val and test set\n",
    "- export test set to csv\n",
    "- delete interim_test_set and test_set from notebook -> helps ensure prevention of data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: 70-30 train-test split, with interim test set\n",
    "train_set, interim_test_set = train_test_split(df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the train set into X_train and y_train\n",
    "X_train = train_set.drop('targetvariable', axis=1)\n",
    "y_train = train_set['targetvariable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second split: 50-50 validation-test split\n",
    "val_set, test_set = train_test_split(interim_test_set, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the validation set into X_val and y_val\n",
    "X_val = train_set.drop('targetvariable', axis=1)\n",
    "y_val = train_set['targetvariable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the test set to a CSV file & delete test set and interim test set\n",
    "test_set.to_csv('test_set.csv', index=False)\n",
    "\n",
    "del test_set, interim_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying different methods of imputation (KNN & RandomForest Imputation with Iterative Imputer) -> see which provides the better result (= the lower mean squared error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_imputation(X, y, imputer, model):\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', imputer),\n",
    "        ('scaler', StandardScaler()),  # Scaling is applied after imputation\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Cross-validate the pipeline\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='neg_mean_squared_error', cv=kf)\n",
    "    \n",
    "    # Return average MSE\n",
    "    return np.mean(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Initialize the Iterative Imputer using RandomForest\n",
    "rf_imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=10, random_state=42), random_state=42)\n",
    "\n",
    "# Initialize the regression model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate KNN imputer\n",
    "mse_knn = evaluate_imputation(X_train, y_train, knn_imputer, model)\n",
    "print(f\"KNN Imputer MSE: {mse_knn}\")\n",
    "\n",
    "# Evaluate Random Forest imputer (IterativeImputer)\n",
    "mse_rf = evaluate_imputation(X_train, y_train, rf_imputer, model)\n",
    "print(f\"Random Forest Imputer MSE: {mse_rf}\")\n",
    "\n",
    "# Determine the best imputer\n",
    "best_imputer = 'KNN' if mse_knn < mse_rf else 'Random Forest'\n",
    "print(f\"Best imputer selected based on cross-validated MSE: {best_imputer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the selected Imputer on the Training & Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO - Implement the best imputer - delete other imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit KNN imputer on the training data\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train_imputed = knn_imputer.fit_transform(X_train)\n",
    "X_val_imputed = knn_imputer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Iterative Imputer using RandomForest on the training data\n",
    "rf_imputer = IterativeImputer(estimator=RandomForestRegressor(n_estimators=10, random_state=42), random_state=42)\n",
    "X_train_imputed = rf_imputer.fit_transform(X_train)\n",
    "X_val_imputed = rf_imputer.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection & Training with TPOT\n",
    "Using TPOTClassifier (Tree-based Pipeline Optimization Tool) to automate the selection and training of the best predictive model based on the cleaned training dataset. This tool explores various models and hyperparameter settings to find the optimal solution. As TPOT can also Impute missing values, I am trying out these two things:\n",
    "- TPOT Classifier on Data without Imputation\n",
    "- TPOT Classifier on Data with Imputation from previous step\n",
    "In order to find out which has better scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT with dataframe with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the TPOT classifier\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, scoring='accuracy', random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier on the validation set\n",
    "print(\"Validation Accuracy: \", tpot.score(X_val, y_val))\n",
    "\n",
    "# Export the best model\n",
    "tpot.export('tpot_best_model.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TPOT with imputed dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the TPOT classifier\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, scoring='accuracy', random_state=42)\n",
    "tpot.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Evaluate the classifier on the validation set\n",
    "print(\"Validation Accuracy: \", tpot.score(X_val_imputed, y_val))\n",
    "\n",
    "# Export the best model\n",
    "tpot.export('tpot_best_model.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving best pipeline from TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = tpot.fitted_pipeline_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "In this step, I analyse the output the best model from the TPOT Classifier using various metrics and visualize results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(y_val, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Neg', 'Predicted Pos'], yticklabels=['Actual Neg', 'Actual Pos'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Actual Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC-AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = model.predict_proba(X_val)[:, 1]  # get the probability of the positive class\n",
    "roc_auc = roc_auc_score(y_val, y_prob)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying False-Positives & False-Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {'Actual': y_val, 'Predicted': y_pred}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identify false positives and false negatives\n",
    "false_positives_val = df[(df['Actual'] == 0) & (df['Predicted'] == 1)]\n",
    "false_negatives_val = df[(df['Actual'] == 1) & (df['Predicted'] == 0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(exported_pipeline.predict, X_train)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# the waterfall_plot shows how we get from shap_values.base_values to model.predict(X)[sample_ind]\n",
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'model' is already trained and 'X_val', 'y_val' are defined\n",
    "result = permutation_importance(model, X_val, y_val, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Get sorted importances\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "# Select top N features\n",
    "top_n = 15  # Adjust N based on your preference\n",
    "sorted_idx_top = sorted_idx[-top_n:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.boxplot(result.importances[sorted_idx_top].T, vert=False, labels=X_val.columns[sorted_idx_top])\n",
    "ax.set_title(\"Permutation Importance of Top Features\")\n",
    "ax.set_xlabel(\"Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a LimeTabularExplainer\n",
    "# Ensure your data is in a suitable format, like a numpy array\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values, \n",
    "    feature_names=X_train.columns.tolist(),\n",
    "    class_names=['No CHD', 'CHD'],  # Update these as per your class labels\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Choose an instance from your validation set to explain\n",
    "i = 10  # Index of the instance in the validation set\n",
    "exp = explainer.explain_instance(X_val.iloc[i], model.predict_proba, num_features=5)\n",
    "\n",
    "# Display the explanation in a Jupyter Notebook\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Identification and Mitigation\n",
    "I am using Fairlearn, a library that was initially developed by researchers at Microsoft. It is now mainained as an open-source project to aid in assessing and mitigating fairness issues in AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fairness_metrics(y_true, y_pred, sensitive_features, sensitive_feature_name):\n",
    "    # Extract the specific sensitive feature\n",
    "    sensitive_feature_data = sensitive_features[sensitive_feature_name]\n",
    "\n",
    "    # Define the metrics you want to check\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score,\n",
    "        'recall': recall_score\n",
    "    }\n",
    "\n",
    "    # Compute metrics\n",
    "    metric_frame = MetricFrame(metrics=metrics,\n",
    "                               y_true=y_true,\n",
    "                               y_pred=y_pred,\n",
    "                               sensitive_features=sensitive_feature_data)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Overall metrics:\")\n",
    "    print(metric_frame.overall)\n",
    "    print(\"\\nMetrics by group:\")\n",
    "    print(metric_frame.by_group)\n",
    "\n",
    "evaluate_fairness_metrics(y_val, y_pred, X_val, 'Ethnicity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographic Parity / Disparity\n",
    "Measures if the decision boundary of the classifier does not vary between groups = groups should receive positive outcomes at equal rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_demographic_parity(predictions, sensitive_features):\n",
    "    \"\"\" Calculate demographic parity disparity\"\"\"\n",
    "    # Convert inputs to pandas Series if they aren't already\n",
    "    if not isinstance(predictions, pd.Series):\n",
    "        predictions = pd.Series(predictions)\n",
    "    if not isinstance(sensitive_features, pd.Series):\n",
    "        sensitive_features = pd.Series(sensitive_features)\n",
    "    \n",
    "    # Combine into a single DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'predictions': predictions,\n",
    "        'sensitive_features': sensitive_features\n",
    "    })\n",
    "    \n",
    "    # Group by the sensitive feature and calculate the mean outcome\n",
    "    parity = data.groupby('sensitive_features')['predictions'].mean()\n",
    "    disparity = np.abs(parity - parity.mean()).max()\n",
    "    return disparity\n",
    "\n",
    "dp_disparity = calculate_demographic_parity(y_pred, X_val['Ethnicity'])\n",
    "print(\"Demographic Parity Disparity:\", dp_disparity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Value = 0: This is the ideal value. It indicates that all groups have the same probability of receiving a positive outcome, fulfilling the criterion of demographic parity perfectly. There is no disparity between the groups in terms of the positive outcome rate.\n",
    "- Value > 0: A nonzero value indicates a disparity in the positive outcome rates across different groups. The closer this value is to 0, the lesser the disparity:\n",
    "\t- Low Disparity (e.g., values closer to 0, like 0.05 or 0.1): Suggests a relatively fair model where the differences in positive outcome rates between groups are minimal.\n",
    "\t- Moderate Disparity (e.g., around 0.2 to 0.3): Indicates a noticeable difference in how groups are treated by the model, which could be a cause for concern and may necessitate further investigation or adjustment of the model.\n",
    "\t- High Disparity (e.g., values approaching 0.5 or higher): Represents significant unfairness, with some groups being substantially more likely to receive positive outcomes than others. This level of disparity is typically unacceptable in practice and requires immediate attention to correct the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equal Opportunity Disparity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_equal_opportunity(predictions, sensitive_features, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate the Equal Opportunity metric.\n",
    "    \"\"\"\n",
    "    # Combine predictions and ground truth\n",
    "    data = pd.DataFrame({'predictions': predictions, 'ground_truth': ground_truth, 'sensitive_features': sensitive_features})\n",
    "    # Function to calculate True Positive Rate\n",
    "    def true_positive_rate(group):\n",
    "        tp = ((group['predictions'] == 1) & (group['ground_truth'] == 1)).sum()\n",
    "        actual_positives = (group['ground_truth'] == 1).sum()\n",
    "        return tp / actual_positives if actual_positives > 0 else 0\n",
    "    \n",
    "    # Calculate TPR by group\n",
    "    tpr = data.groupby('sensitive_features').apply(true_positive_rate)\n",
    "    disparity = np.abs(tpr - tpr.mean()).max()\n",
    "    return disparity\n",
    "\n",
    "eo_disparity = calculate_equal_opportunity(pd.Series(y_pred, name='predictions'), pd.Series(X_val['Ethnicity'], name='sensitive_features'), pd.Series(y_val, name='ground_truth'))\n",
    "\n",
    "print(\"Equal Opportunity Disparity:\", eo_disparity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Value = 0: This value indicates perfect fairness in terms of equal opportunity. It means that all groups have the same true positive rate, i.e., the model is equally likely to correctly predict positive outcomes across all sensitive groups.\n",
    "- Value > 0: Indicates that there’s a difference in the true positive rates among the groups:\n",
    "  - Low Disparity (e.g., values closer to 0, like 0.05 or 0.1): Suggests that the model is relatively fair, with minimal differences in how accurately it predicts positive outcomes across different groups.\n",
    "\t- Moderate Disparity (e.g., around 0.15 to 0.25): This level of disparity points to a moderate level of unfairness. The model is less accurate in predicting positive outcomes for some groups compared to others, which might raise concerns depending on the application context.\n",
    "\t- High Disparity (e.g., values approaching 0.3 or higher): Indicates a significant level of unfairness, showing that the model performs considerably better for certain groups than for others when predicting positive outcomes. This is generally unacceptable and requires adjustments to the model or its training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equality of Odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_equality_of_odds(predictions, sensitive_features, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate the Equality of Odds metric.\n",
    "    It includes both True Positive Rate (TPR) and False Positive Rate (FPR) disparities.\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame({\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth,\n",
    "        'sensitive_features': sensitive_features\n",
    "    })\n",
    "    \n",
    "    def odds_rates(group):\n",
    "        tn, fp, fn, tp = confusion_matrix(group['ground_truth'], group['predictions']).ravel()\n",
    "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) != 0 else 0\n",
    "        return pd.Series({'TPR': tpr, 'FPR': fpr})\n",
    "    \n",
    "    odds = data.groupby('sensitive_features').apply(odds_rates)\n",
    "    tpr_disparity = np.abs(odds['TPR'] - odds['TPR'].mean()).max()\n",
    "    fpr_disparity = np.abs(odds['FPR'] - odds['FPR'].mean()).max()\n",
    "    return tpr_disparity, fpr_disparity\n",
    "\n",
    "\n",
    "tpr_disparity, fpr_disparity = calculate_equality_of_odds(y_pred, X_val['Ethnicity'], y_val)\n",
    "print(\"TPR Disparity:\", tpr_disparity)\n",
    "print(\"FPR Disparity:\", fpr_disparity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Accuracy Disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_overall_accuracy_equality(predictions, sensitive_features, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate Overall Accuracy Equality across different groups.\n",
    "    \"\"\"\n",
    "    data = pd.DataFrame({\n",
    "        'predictions': predictions,\n",
    "        'ground_truth': ground_truth,\n",
    "        'sensitive_features': sensitive_features\n",
    "    })\n",
    "    \n",
    "    def accuracy(group):\n",
    "        correct = (group['predictions'] == group['ground_truth']).sum()\n",
    "        total = group.shape[0]\n",
    "        return correct / total\n",
    "    \n",
    "    accuracies = data.groupby('sensitive_features').apply(accuracy)\n",
    "    accuracy_disparity = np.abs(accuracies - accuracies.mean()).max()\n",
    "    return accuracy_disparity\n",
    "\n",
    "\n",
    "accuracy_disparity = calculate_overall_accuracy_equality(y_pred, X_val['Ethnicity'], y_val)\n",
    "print(\"Overall Accuracy Disparity:\", accuracy_disparity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE (Oversampling)\n",
    "Oversamples the minority class with synthetic samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Applying SMOTE\u001b[39;00m\n\u001b[1;32m      2\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m X_smote, y_smote \u001b[38;5;241m=\u001b[39m smote\u001b[38;5;241m.\u001b[39mfit_resample(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal dataset shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCounter(y_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResampled dataset shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCounter(y_smote)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Applying SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f'Original dataset shape {Counter(y_train)}')\n",
    "print(f'Resampled dataset shape {Counter(y_smote)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NearMiss (Undersampling)\n",
    "Undersamples instances that are particularly close to instances of the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying NearMiss\n",
    "nm = NearMiss(version=1)\n",
    "X_nm, y_nm = nm.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f'Original dataset shape {Counter(y_train)}')\n",
    "print(f'Resampled dataset shape {Counter(y_nm)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class-weight Balancing\n",
    "Balances the weight of the individual classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
